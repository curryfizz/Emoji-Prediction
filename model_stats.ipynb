{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras.models import load_model, model_from_json\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"models/training set v.3.0\"\n",
    "\n",
    "model_name = \"epochs_100_layers_30_40_20_10_5\"\n",
    "\n",
    "model_json_path = f\"{model_folder}/json/{model_name}.json\"\n",
    "\n",
    "model_weight_path = f\"{model_folder}/weights/{model_name}.h5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# model = load_model(f\"{model_folder}{model_name}.{model_extension}\")\n",
    "# model = keras.models.model_from_json(f\"{model_folder}{model_name}.{model_extension}\")\n",
    "\n",
    "\n",
    "json_file = open(f\"{model_json_path}\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(model_weight_path)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get emoji from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\n",
    "    0: \":red_heart:\",  # Love # X\n",
    "    1: \":face_with_tears_of_joy:\",  # Laughter X\n",
    "    2: \":grinning_face_with_big_eyes:\", # Happiness # X\n",
    "    3: \":loudly_crying_face:\",  # Sadness # X\n",
    "    4: \":smiling_face_with_heart-eyes:\",  # Adoration X\n",
    "    5: \":fire:\",  # Excitement X\n",
    "    6: \":thumbs_up:\",  # Approval\n",
    "    7: \":folded_hands:\",  # Gratitude X\n",
    "    8: \":angry_face:\",  # Anger X\n",
    "    9: \":thinking_face:\",  # Contemplation X\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "    \"I am seeing what is happening\",\n",
    "    \"Does loading and unloading even work?\",\n",
    "    \"Naniii\",\n",
    "    \"Wtf is happening\",\n",
    "    \"Im dying\",\n",
    "    \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n",
    "    \"I like your hoodie\",\n",
    "    \"Thank you for the help\",\n",
    "    \"Your jokes are so funny\",\n",
    "    \"I made it\",\n",
    "    \"You are so hot\",\n",
    "    \"I love you so much\",\n",
    "    \"Good job!\",\n",
    "    \"I am trying\", \"I want to cry\", \"This is just sad\",\n",
    "    \"I love sleeeping :)\",\n",
    "    \"Wait what is the syllabus even rip\",\n",
    "    \"slayyy\",\n",
    "    \"FIRE\",\n",
    "    \"If you're happy and you know it clap your hands\"\n",
    "]\n",
    "\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test)\n",
    "Xtest = pad_sequences(test_seq, maxlen=loaded_model.layers[0].input_shape[1], padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 9, 9, 1, 9, 3, 7, 1, 3, 0, 0, 6, 3, 3, 3, 0, 9, 9, 5, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = loaded_model.predict(Xtest)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am seeing what is happening ðŸ˜­\n",
      "Does loading and unloading even work? ðŸ˜­\n",
      "Naniii ðŸ¤”\n",
      "Wtf is happening ðŸ¤”\n",
      "Im dying ðŸ˜‚\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA ðŸ¤”\n",
      "I like your hoodie ðŸ˜­\n",
      "Thank you for the help ðŸ™\n",
      "Your jokes are so funny ðŸ˜‚\n",
      "I made it ðŸ˜­\n",
      "You are so hot â¤ï¸\n",
      "I love you so much â¤ï¸\n",
      "Good job! ðŸ‘\n",
      "I am trying ðŸ˜­\n",
      "I want to cry ðŸ˜­\n",
      "This is just sad ðŸ˜­\n",
      "I love sleeeping :) â¤ï¸\n",
      "Wait what is the syllabus even rip ðŸ¤”\n",
      "slayyy ðŸ¤”\n",
      "FIRE ðŸ”¥\n",
      "If you're happy and you know it clap your hands ðŸ˜‚\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test)):\n",
    "    print(test[i], label_to_emoji(y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (21,) (16,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m)\n\u001b[0;32m      5\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_labels)\n\u001b[0;32m      6\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m total_predictions\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (21,) (16,) "
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "test_labels = [9,3,8,8,3,3,4,7,1,2,5,0,6,3,3,3]\n",
    "\n",
    "correct_predictions = np.sum(y_pred == test_labels)\n",
    "total_predictions = len(test_labels)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "\n",
    "# Calculate loss\n",
    "# Assuming you have a loss function defined or you're using a standard loss function like categorical crossentropy\n",
    "loss = 0.0\n",
    "for i in range(len(test_labels)):\n",
    "    true_label = test_labels[i]\n",
    "    predicted_value = y_pred[i]  \n",
    "    loss += (true_label - predicted_value) ** 2\n",
    "loss /= len(test_labels)\n",
    "print(f'Test loss (MSE): {loss}')\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
