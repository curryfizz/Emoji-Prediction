{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Embedding\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split  # Import for splitting the data\n",
    "from gensim.models import KeyedVectors\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woke up feelin' dangerous</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Got my mind on my money</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haha that was hilarious</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feeling like Uptown Funk</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Congratulations for having a baby</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0  1\n",
       "0          Woke up feelin' dangerous  5\n",
       "1            Got my mind on my money  5\n",
       "2            Haha that was hilarious  1\n",
       "3           Feeling like Uptown Funk  5\n",
       "4  Congratulations for having a baby  2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/emoji_data/emoji_data.csv\", header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emoji Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"emoji_dictionary = {\n",
    "    0: \":red_heart:\",  # Love\n",
    "    1: \":face_with_tears_of_joy:\",  # Laughter\n",
    "    2: \":smiling_face_with_heart-eyes:\",  # Adoration\n",
    "    3: \":loudly_crying_face:\",  # Sadness\n",
    "    4: \":fire:\",  # Excitement\n",
    "    5: \":thumbs_up:\",  # Approval\n",
    "    6: \":folded_hands:\",  # Gratitude\n",
    "    7: \":angry_face:\",  # Anger\n",
    "    8: \":sparkles:\",  # Happiness\n",
    "    9: \":weary_face:\",  # Exhaustion\n",
    "    10: \":astonished_face:\",  # Surprise\n",
    "    11: \":confused_face:\",  # Confusion\n",
    "    12: \":tropical_drink:\",  # Celebration\n",
    "    13: \":broken_heart:\",  # Heartbreak\n",
    "    14: \":thinking_face:\",  # Contemplation\n",
    "    15: \":sleeping_face:\",  # Sleepiness\n",
    "    16: \":victory_hand:\",  # Success\n",
    "    17: \":thumbs_down:\",  # Disapproval\n",
    "    18: \":grimacing_face:\",  # Discomfort\n",
    "    19: \":smiling_face_with_halo:\",  # Innocence\n",
    "}\"\"\"\n",
    "\n",
    "emoji_dictionary = {\n",
    "    0: \":red_heart:\",  # Love #\n",
    "    1: \":face_with_tears_of_joy:\",  # Laughter\n",
    "    2: \":grinning_face_with_big_eyes:\", # Happiness #\n",
    "    3: \":loudly_crying_face:\",  # Sadness #\n",
    "    4: \":smiling_face_with_heart-eyes:\",  # Adoration\n",
    "    5: \":fire:\",  # Excitement\n",
    "    6: \":thumbs_up:\",  # Approval\n",
    "    7: \":folded_hands:\",  # Gratitude\n",
    "    8: \":angry_face:\",  # Anger\n",
    "    9: \":thinking_face:\",  # Contemplation\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get labels from CLDR names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woke up feelin' dangerous</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Got my mind on my money</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haha that was hilarious</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feeling like Uptown Funk</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Congratulations for having a baby</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0  1\n",
       "0          Woke up feelin' dangerous  5\n",
       "1            Got my mind on my money  5\n",
       "2            Haha that was hilarious  1\n",
       "3           Feeling like Uptown Funk  5\n",
       "4  Congratulations for having a baby  2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/emoji_data/emoji_data.csv', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[0].values\n",
    "Y = data[1].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With glove dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# with open('data/glove_dataset/glove.6B.100d.txt','r', encoding='utf8') as file:\n",
    "#     content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = {}\n",
    "\n",
    "# for line in content:\n",
    "#     line = line.split()\n",
    "#     embeddings[line[0]] = np.array(line[1:], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With crawl dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model_path = \"data/fast_text/crawl_dataset/crawl-300d-2M-subword.vec\"\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert FastText embeddings to a dictionary\n",
    "embeddings = {}\n",
    "for word in fasttext_model.index_to_key:\n",
    "    embeddings[word] = fasttext_model.get_vector(word)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert input text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_to_index = tokenizer.word_index\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtokens = tokenizer.texts_to_sequences(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maxlen(data):\n",
    "    maxlen = 0\n",
    "    for sent in data:\n",
    "        maxlen = max(maxlen, len(sent))\n",
    "    \n",
    "    return maxlen\n",
    "maxlen = get_maxlen(Xtokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = pad_sequences(Xtokens, maxlen=maxlen, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_to_index) + 1, embed_size))\n",
    "\n",
    "for word, i in word_to_index.items():\n",
    "    if word in embeddings:\n",
    "        embed_vector = embeddings[word]\n",
    "        embedding_matrix[i] = embed_vector\n",
    "    else:\n",
    "        # Handle out-of-vocabulary words or phrases by aggregating subword embeddings\n",
    "        phrase_embed_sum = None\n",
    "        clean_word = re.split(r'\\W+', word)\n",
    "        for subword in clean_word:\n",
    "            if subword in embeddings:\n",
    "                if phrase_embed_sum is None:\n",
    "                    phrase_embed_sum = embeddings[subword].copy()\n",
    "                else:\n",
    "                    phrase_embed_sum += embeddings[subword]\n",
    "        # Assign the aggregated embedding to a new array with the desired dtype\n",
    "        if phrase_embed_sum is not None:\n",
    "            embedding_matrix[i] = phrase_embed_sum / len(clean_word)\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general more layers >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "#units\n",
    "layers = [256, 16, 4, 2]\n",
    "\n",
    "num_layers = len(layers)\n",
    "\n",
    "\n",
    "num_classes = len(emoji_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate file name for saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def returnfilename():\n",
    "    layers_str = '_'.join(map(str, layers))  # Convert layers to string and join them with '_'\n",
    "    return f\"epochs_{epochs}_layers_{layers_str}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add Embedding layer\n",
    "model.add(Embedding(input_dim=len(word_to_index) + 1,\n",
    "                    output_dim=embed_size,\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False))\n",
    "\n",
    "\n",
    "# Add LSTM layers\n",
    "for i in range(num_layers):\n",
    "    # For the last layer, return_sequences=False\n",
    "    if i == num_layers - 1:\n",
    "        model.add(LSTM(units=layers[i]))\n",
    "    else:\n",
    "        model.add(LSTM(units=layers[i], return_sequences=True))\n",
    "\n",
    "# Add output Dense layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_crossentropy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 7s 19ms/step - loss: 2.3002 - categorical_crossentropy: 2.3002\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 2.2873 - categorical_crossentropy: 2.2873\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 2.2453 - categorical_crossentropy: 2.2453\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 2.1671 - categorical_crossentropy: 2.1671\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 2.1121 - categorical_crossentropy: 2.1121\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 2.0771 - categorical_crossentropy: 2.0771\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.9921 - categorical_crossentropy: 1.9921\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 1.9892 - categorical_crossentropy: 1.9892\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.8889 - categorical_crossentropy: 1.8889\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.8434 - categorical_crossentropy: 1.8434\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.7841 - categorical_crossentropy: 1.7841\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.7312 - categorical_crossentropy: 1.7312\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.6910 - categorical_crossentropy: 1.6910\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 1.6991 - categorical_crossentropy: 1.6991\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.6882 - categorical_crossentropy: 1.6882\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.6001 - categorical_crossentropy: 1.6001\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.5719 - categorical_crossentropy: 1.5719\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.5032 - categorical_crossentropy: 1.5032\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.4978 - categorical_crossentropy: 1.4978\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.4518 - categorical_crossentropy: 1.4518\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.4097 - categorical_crossentropy: 1.4097\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.3774 - categorical_crossentropy: 1.3774\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.3986 - categorical_crossentropy: 1.3986\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.3786 - categorical_crossentropy: 1.3786\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 1.3980 - categorical_crossentropy: 1.3980\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.3812 - categorical_crossentropy: 1.3812\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.3202 - categorical_crossentropy: 1.3202\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.2842 - categorical_crossentropy: 1.2842\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.2663 - categorical_crossentropy: 1.2663\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.2614 - categorical_crossentropy: 1.2614\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.2323 - categorical_crossentropy: 1.2323\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.1861 - categorical_crossentropy: 1.1861\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.1766 - categorical_crossentropy: 1.1766\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.1533 - categorical_crossentropy: 1.1533\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 1.1619 - categorical_crossentropy: 1.1619\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.1451 - categorical_crossentropy: 1.1451\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.1613 - categorical_crossentropy: 1.1613\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.1255 - categorical_crossentropy: 1.1255\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.0900 - categorical_crossentropy: 1.0900\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.0803 - categorical_crossentropy: 1.0803\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 1.1323 - categorical_crossentropy: 1.1323\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.1143 - categorical_crossentropy: 1.1143\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.0523 - categorical_crossentropy: 1.0523\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.0271 - categorical_crossentropy: 1.0271\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 1.0195 - categorical_crossentropy: 1.0195\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.0094 - categorical_crossentropy: 1.0094\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.0132 - categorical_crossentropy: 1.0132\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.9646 - categorical_crossentropy: 0.9646\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 1.0109 - categorical_crossentropy: 1.0109\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 1.0515 - categorical_crossentropy: 1.0515\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.9575 - categorical_crossentropy: 0.9575\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.9516 - categorical_crossentropy: 0.9516\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.9107 - categorical_crossentropy: 0.9107\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8744 - categorical_crossentropy: 0.8744\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.8669 - categorical_crossentropy: 0.8669\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8761 - categorical_crossentropy: 0.8761\n",
      "Epoch 57/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8788 - categorical_crossentropy: 0.8788\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8580 - categorical_crossentropy: 0.8580\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8680 - categorical_crossentropy: 0.8680\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8988 - categorical_crossentropy: 0.8988\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8618 - categorical_crossentropy: 0.8618\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8413 - categorical_crossentropy: 0.8413\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8157 - categorical_crossentropy: 0.8157\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8895 - categorical_crossentropy: 0.8895\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8167 - categorical_crossentropy: 0.8167\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8233 - categorical_crossentropy: 0.8233\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.8107 - categorical_crossentropy: 0.8107\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7969 - categorical_crossentropy: 0.7969\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.7724 - categorical_crossentropy: 0.7724\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7686 - categorical_crossentropy: 0.7686\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7503 - categorical_crossentropy: 0.7503\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7513 - categorical_crossentropy: 0.7513\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.7639 - categorical_crossentropy: 0.7639\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.7380 - categorical_crossentropy: 0.7380\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.7471 - categorical_crossentropy: 0.7471\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7316 - categorical_crossentropy: 0.7316\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.6970 - categorical_crossentropy: 0.6970\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.7365 - categorical_crossentropy: 0.7365\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8075 - categorical_crossentropy: 0.8075\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7544 - categorical_crossentropy: 0.7544\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7468 - categorical_crossentropy: 0.7468\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.7241 - categorical_crossentropy: 0.7241\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.6851 - categorical_crossentropy: 0.6851\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.6804 - categorical_crossentropy: 0.6804\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.6824 - categorical_crossentropy: 0.6824\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.6557 - categorical_crossentropy: 0.6557\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.6372 - categorical_crossentropy: 0.6372\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.6381 - categorical_crossentropy: 0.6381\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.6495 - categorical_crossentropy: 0.6495\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.6727 - categorical_crossentropy: 0.6727\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7912 - categorical_crossentropy: 0.7912\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8525 - categorical_crossentropy: 0.8525\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7365 - categorical_crossentropy: 0.7365\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.6862 - categorical_crossentropy: 0.6862\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.6378 - categorical_crossentropy: 0.6378\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.6354 - categorical_crossentropy: 0.6354\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.6233 - categorical_crossentropy: 0.6233\n",
      "Epoch 98/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.6002 - categorical_crossentropy: 0.6002\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.5973 - categorical_crossentropy: 0.5973\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.6175 - categorical_crossentropy: 0.6175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2804b9e2860>"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain, epochs=epochs,  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "filename = returnfilename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "model_save_path = \"models/training set v.3.0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(f\"{model_save_path}/json/{filename}.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(f\"{model_save_path}/weights/{filename}.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "metrics_folder = f\"{model_save_path}/metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 8ms/step - loss: 0.4412 - categorical_crossentropy: 0.4412\n",
      "Test Loss: 0.44121840596199036\n",
      "Test Accuracy: 0.44121840596199036\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "current_datetime = datetime.now()\n",
    "formatted_datetime = current_datetime.strftime('%d %B, %Y')\n",
    "\n",
    "# Write model architecture and details to file\n",
    "with open(f\"{metrics_folder}/{filename}.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"# Model description:<br>\\n\")\n",
    "    file.write(f\"Date generated: {formatted_datetime}<br>\\n\")\n",
    "    file.write(f\"Number of classes: {num_classes}<br>\\n\")\n",
    "    file.write(f\"Number of samples: {len(X)}<br>\\n\")\n",
    "    file.write(f\"Training set size: {len(X_train)}<br>\\n\")\n",
    "    file.write(f\"Test set size: {len(X_test)}<br>\\n\")\n",
    "    file.write(f\"Epochs used: {epochs}<br>\\n\")\n",
    "    file.write(f\"Number of layers: {num_layers}<br>\\n\")\n",
    "    file.write(f\"Layers used: {layers}<br>\\n\")\n",
    "    file.write(\"Optimizer: Adam<br>\\n\")\n",
    "    file.write(\"Loss function: Categorical Crossentropy<br>\\n\")\n",
    "    file.write(\"# Evaluation Results<br>\\n\")\n",
    "    file.write(f\"Test Loss: {loss:.4f}<br>\\n\")\n",
    "    file.write(f\"Test Accuracy: {accuracy:.4f}<br><br>\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 6ms/step\n",
      "Accuracy: 0.8431372549019608\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 😭 Actual: 😃\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: ❤️ Actual: 🙏\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🔥 Actual: 😃\n",
      "Predicted: 👍 Actual: 🙏\n",
      "Predicted: 🔥 Actual: 😃\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🤔 Actual: ❤️\n",
      "Predicted: ❤️ Actual: 🙏\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 😃 Actual: 🔥\n",
      "Predicted: 🤔 Actual: 😠\n",
      "Predicted: 🔥 Actual: 😃\n",
      "Predicted: ❤️ Actual: 😠\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predicted and actual labels from one-hot encoded format to integer labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "Y_test_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compare predicted labels with actual labels\n",
    "correct_predictions = np.sum(y_pred_labels == Y_test_labels)\n",
    "total_predictions = len(Y_test_labels)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Write accuracy to file\n",
    "with open(f\"{metrics_folder}/{filename}.md\", \"a\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"Accuracy: {:.4f}\\n\\n\".format(accuracy))\n",
    "\n",
    "\n",
    "# Write misclassified samples to file\n",
    "with open(f\"{metrics_folder}/{filename}.md\", \"a\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"## Misclassified samples:<br>\\n\")\n",
    "    for i in range(len(y_pred_labels)):\n",
    "        if y_pred_labels[i] != Y_test_labels[i]:\n",
    "            file.write(\n",
    "                f\"Predicted: {label_to_emoji(y_pred_labels[i])} Actual: {label_to_emoji(Y_test_labels[i])}<br>\\n\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_pred_labels)):\n",
    "    if y_pred_labels[i]!=Y_test_labels[i]:\n",
    "        print(\"Predicted:\", label_to_emoji(y_pred_labels[i]), \"Actual:\", label_to_emoji(Y_test_labels[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    }
   ],
   "source": [
    "test = [\"I am trying\", \"I want to cry\", \"This is just sad\"]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test)\n",
    "Xtest = pad_sequences(test_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "y_pred = model.predict(Xtest)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Write predictions to file\n",
    "with open(f\"{metrics_folder}/{filename}.md\", \"a\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n## Random test predictions: (accuracy based on discussion)<br>\\n\")\n",
    "    for i in range(len(test)):\n",
    "        file.write(f\"{test[i]} {label_to_emoji(y_pred[i])}<br>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am trying 😭\n",
      "I want to cry 😭\n",
      "This is just sad 😭\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test)):\n",
    "    print(test[i], label_to_emoji(y_pred[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
