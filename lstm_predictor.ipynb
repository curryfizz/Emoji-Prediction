{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Embedding\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split  # Import for splitting the data\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When your alarm goes off for the fifth time</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>That moment when someone eats the last slice o...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you finally finish a project</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me trying to understand the group chat</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When the music is too loud at the party</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   1\n",
       "0       When your alarm goes off for the fifth time    9\n",
       "1  That moment when someone eats the last slice o...   7\n",
       "2                 When you finally finish a project    8\n",
       "3            Me trying to understand the group chat   11\n",
       "4           When the music is too loud at the party   18"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/emoji_data/emoji_data.csv', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emoji Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\n",
    "    0: \":red_heart:\",  # Love\n",
    "    1: \":face_with_tears_of_joy:\",  # Laughter\n",
    "    2: \":smiling_face_with_heart-eyes:\",  # Adoration\n",
    "    3: \":loudly_crying_face:\",  # Sadness\n",
    "    4: \":fire:\",  # Excitement\n",
    "    5: \":thumbs_up:\",  # Approval\n",
    "    6: \":folded_hands:\",  # Gratitude\n",
    "    7: \":angry_face:\",  # Anger\n",
    "    8: \":sparkles:\",  # Happiness\n",
    "    9: \":weary_face:\",  # Exhaustion\n",
    "    10: \":astonished_face:\",  # Surprise\n",
    "    11: \":confused_face:\",  # Confusion\n",
    "    12: \":tropical_drink:\",  # Celebration\n",
    "    13: \":broken_heart:\",  # Heartbreak\n",
    "    14: \":thinking_face:\",  # Contemplation\n",
    "    15: \":sleeping_face:\",  # Sleepiness\n",
    "    16: \":victory_hand:\",  # Success\n",
    "    17: \":thumbs_down:\",  # Disapproval\n",
    "    18: \":grimacing_face:\",  # Discomfort\n",
    "    19: \":smiling_face_with_halo:\",  # Innocence\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['When your alarm goes off for the fifth time ',\n",
       "       'That moment when someone eats the last slice of pizza ',\n",
       "       'When you finally finish a project ',\n",
       "       'Me trying to understand the group chat ',\n",
       "       'When the music is too loud at the party ',\n",
       "       'That feeling when you see your crush ',\n",
       "       'When dessert ruins your healthy eating plans ',\n",
       "       'Waking up on Monday morning ',\n",
       "       \"Pretending to laugh at your boss's joke \",\n",
       "       'Pretending to listen to a long story ',\n",
       "       \"Finding out there's free food at work \", 'Stuck in traffic ',\n",
       "       'Treating yourself even when broke ',\n",
       "       'Trying to adult but failing ',\n",
       "       'Trying to stay awake during a boring lecture ',\n",
       "       \"Realizing it's only Wednesday \",\n",
       "       'Trying to be positive but life keeps testing you ',\n",
       "       'Making plans for the weekend but ending up in bed ',\n",
       "       'Being the only one not having fun at a party ',\n",
       "       \"Accidentally liking someone's old post \",\n",
       "       'Figuring out what to wear in the morning ',\n",
       "       'Pretending to be interested in a boring story ',\n",
       "       'Forgetting to do something important ',\n",
       "       'Trying to get through the week ',\n",
       "       'Holding back tears but failing ',\n",
       "       'Finally relaxing after finishing chores ',\n",
       "       'Staying calm in a stressful situation ',\n",
       "       'Being productive but procrastinating instead ',\n",
       "       'Realizing you sent a text to the wrong person ',\n",
       "       'Resisting the urge to buy something unnecessary ',\n",
       "       'Trying to be positive but everything goes wrong ',\n",
       "       'Stuck in a never-ending meeting ',\n",
       "       'Being patient when someone is testing your limits ',\n",
       "       'Hiding disappointment behind a smile ',\n",
       "       'Realizing you have to wake up early on a weekend ',\n",
       "       \"Pretending to be excited about something you don't care about \",\n",
       "       'Trying to act cool but secretly freaking out inside ',\n",
       "       'Monday morning struggles ',\n",
       "       'When your phone dies mid-conversation ',\n",
       "       'Pretending to laugh at a bad joke ',\n",
       "       'Trying to act cool in front of your crush ',\n",
       "       \"When you accidentally like your ex's post \",\n",
       "       'Getting stuck in an awkward silence ',\n",
       "       'When you try to parallel park and fail ',\n",
       "       'Trying to look busy at work ',\n",
       "       'Realizing you forgot your lunch at home ',\n",
       "       'Sneezing in public and feeling judged ',\n",
       "       \"When you think it's Friday but it's only Thursday \",\n",
       "       'Waking up with bedhead and no time to fix it ',\n",
       "       \"When you forget someone's name right after meeting them \",\n",
       "       'Trying to resist hitting the snooze button ',\n",
       "       \"When you're hungry but can't decide what to eat \",\n",
       "       'When your WiFi suddenly stops working ',\n",
       "       'Accidentally sending a text to your boss instead of your friend ',\n",
       "       'Realizing you missed your favorite TV show ',\n",
       "       \"When you think it's Friday but it's only Monday \",\n",
       "       'Trying to hold back a yawn in a meeting ',\n",
       "       'Forgetting your umbrella on a rainy day ',\n",
       "       'When autocorrect changes your text to something embarrassing ',\n",
       "       'Sneezing multiple times in a row ',\n",
       "       \"When you're late for work and can't find your keys \",\n",
       "       'Accidentally dropping your phone face down ',\n",
       "       'When you see a spider in your room ',\n",
       "       'Forgetting your lunch and having to buy expensive cafeteria food ',\n",
       "       'When your friend cancels plans at the last minute ',\n",
       "       'Trying to make a decision but ending up more confused ',\n",
       "       'Realizing you left your wallet at home while grocery shopping ',\n",
       "       'When you accidentally hit \"reply all\" in an email ',\n",
       "       \"Pretending to understand a joke you didn't hear \",\n",
       "       'Trying to take a cute selfie and failing ',\n",
       "       'When your favorite song comes on the radio ',\n",
       "       'Getting a cramp while working out ',\n",
       "       \"When you're craving a snack but have nothing in the house \",\n",
       "       'Waking up to the sound of construction outside ',\n",
       "       'When your computer freezes in the middle of an important task ',\n",
       "       'Realizing you have no clean socks left ',\n",
       "       'Another Monday another thrilling start to the week',\n",
       "       'Who needs an umbrella when you have the power of optimism?',\n",
       "       'Plans canceled? Oh what a surprise!',\n",
       "       'Decoding the mysteries of pet behavior',\n",
       "       'Operating at peak brain capacity',\n",
       "       'Accidentally using the perfect emoji once again',\n",
       "       'Boss text mishap strikes again',\n",
       "       \"Logical reasoning? Nah I'll pass.\",\n",
       "       'Achieving superhero status one mundane task at a time',\n",
       "       'Craving ice cream at midnight? Well obviously.',\n",
       "       'Just another flawless interaction with my crush',\n",
       "       'Making up lyrics because who needs accuracy?',\n",
       "       'Reliving embarrassing moments in exquisite detail',\n",
       "       'Cereal: the dinner of champions.',\n",
       "       'Left on read again? Shocking I know.',\n",
       "       'Just another day in the endless abyss of the week',\n",
       "       \"Cracking the code of my friend's unique humor\",\n",
       "       \"Accidentally liking someone's post from 2015? Classic.\",\n",
       "       'Suppressing laughter: an art form mastered.',\n",
       "       'Naps: because who needs energy?',\n",
       "       'No such thing as too much regret over pizza choices.',\n",
       "       'Another exam another energy drink.',\n",
       "       'Who needs sleep? Energy drinks got me covered.',\n",
       "       'Exams canceled? More time for caffeine.',\n",
       "       'Exam questions: decipher or die.', 'Caffeine-fueled desperation.',\n",
       "       'Accidentally sent my energy drink order to the professor. Oops.',\n",
       "       'Boss text mishap. Just add it to the chaos.',\n",
       "       'Logical reasoning? More like panic mode.',\n",
       "       'Surviving on caffeine and hope.',\n",
       "       'Midnight craving: energy drinks. Sleep can wait.',\n",
       "       'Library vibes: energy drinks and anxiety.',\n",
       "       'Excuse for missing class: energy drink overdose.',\n",
       "       'Spilled energy drink on my exam. Great start.',\n",
       "       'Energy drinks: student fuel.',\n",
       "       'Left on read again? Story of my life.',\n",
       "       'Balancing exams and caffeine crashes.',\n",
       "       'Cracking open another can of energy drink.',\n",
       "       'Regretting energy drinks before bed.',\n",
       "       'Suppressing a caffeine-induced panic attack.',\n",
       "       'Naps: escape from the caffeine jitters.',\n",
       "       'No such thing as too much caffeine regret.',\n",
       "       \"Bae's text got me feeling all\",\n",
       "       'Laughing so hard tears streaming down my face',\n",
       "       'That sunset view got me like',\n",
       "       'Feeling broken inside but pretending to be okay',\n",
       "       'This party is lit',\n",
       "       'Giving your friend a thumbs up for that sick dance move',\n",
       "       'Sending prayers and good vibes',\n",
       "       \"When you're angry but trying to keep calm\",\n",
       "       'Just found out I aced the test!',\n",
       "       'Feeling astonished by the surprise birthday party',\n",
       "       \"Wait what? I'm so confused right now\",\n",
       "       'Time to celebrate with a tropical drink!',\n",
       "       'Heartbroken over the breakup',\n",
       "       \"Contemplating life's mysteries...\",\n",
       "       'Ready to crash after a long day',\n",
       "       'Victory dance because I nailed the presentation!',\n",
       "       'Disapproving of that awful movie',\n",
       "       'This discomfort is killing me',\n",
       "       'Feeling innocent in this chaotic world', 'OMG so cringe rn',\n",
       "       'That view tho', 'Feeling low-key shook', \"This party's lit af\",\n",
       "       'Giving props for that sick move', 'Sending good vibes your way',\n",
       "       \"When you're low-key angry but tryna chill\",\n",
       "       \"Just aced that test let's flex\", 'Shook by the surprise party',\n",
       "       'Wait what? So confused rn',\n",
       "       'Time to turn up with a tropical drink',\n",
       "       'Heartbroken over the breakup', \"Contemplating life's mysteries\",\n",
       "       'Ready to crash after a long day',\n",
       "       'Victory dance for nailing the presentation',\n",
       "       'Disapproving of that movie like', 'This discomfort is too much',\n",
       "       'Feeling innocent in this mess', 'I cry', 'Pajn', 'Mondays = pajn',\n",
       "       'i smol', 'ion understand anything'], dtype=object)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[0].values\n",
    "Y = data[1].values\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/glove_dataset/glove.6B.100d.txt','r', encoding='utf8') as file:\n",
    "#     content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fasttext_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/fast_text/crawl_dataset/crawl-300d-2M-subword.vec\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m fasttext_model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfasttext_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nawsh\\miniconda3\\envs\\myenv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nawsh\\miniconda3\\envs\\myenv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[0;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2067\u001b[0m         )\n\u001b[0;32m   2068\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2069\u001b[0m         \u001b[43m_word2vec_read_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(kv):\n\u001b[0;32m   2071\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2073\u001b[0m         kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(kv),\n\u001b[0;32m   2074\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nawsh\\miniconda3\\envs\\myenv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1971\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_word2vec_read_text\u001b[39m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding):\n\u001b[0;32m   1970\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(vocab_size):\n\u001b[1;32m-> 1971\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[43mfin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1972\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1973\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fasttext_model_path = \"data/fast_text/crawl_dataset/crawl-300d-2M-subword.vec\"\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert FastText embeddings to a dictionary\n",
    "embeddings = {}\n",
    "for word in fasttext_model.index_to_key:\n",
    "    embeddings[word] = fasttext_model.get_vector(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = {}\n",
    "\n",
    "# for line in content:\n",
    "#     line = line.split()\n",
    "#     embeddings[line[0]] = np.array(line[1:], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert input text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_to_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtokens = tokenizer.texts_to_sequences(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_maxlen(data):\n",
    "    maxlen = 0\n",
    "    for sent in data:\n",
    "        maxlen = max(maxlen, len(sent))\n",
    "    \n",
    "    return maxlen\n",
    "maxlen = get_maxlen(Xtokens)\n",
    "\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = pad_sequences(Xtokens, maxlen=maxlen, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = to_categorical(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "128\n",
      "32\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(X_test))\n",
    "print(len(X_train))\n",
    "print(len(Y_test))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_to_index) + 1, embed_size))\n",
    "\n",
    "for word, i in word_to_index.items():\n",
    "    if word in embeddings:\n",
    "        embed_vector = embeddings[word]\n",
    "        embedding_matrix[i] = embed_vector\n",
    "    else:\n",
    "        # Handle out-of-vocabulary words or phrases by aggregating subword embeddings\n",
    "        phrase_embed_sum = None\n",
    "        for subword in word.split():\n",
    "            if subword in embeddings:\n",
    "                if phrase_embed_sum is None:\n",
    "                    phrase_embed_sum = embeddings[subword]\n",
    "                else:\n",
    "                    phrase_embed_sum += embeddings[subword]\n",
    "        if phrase_embed_sum is not None:\n",
    "            # Take the average of subword embeddings\n",
    "            embedding_matrix[i] = phrase_embed_sum / len(word.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_to_index) + 1,\n",
    "              output_dim=embed_size,\n",
    "              input_length=maxlen,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False),\n",
    "    LSTM(units=50, return_sequences=True),\n",
    "    LSTM(units=50),\n",
    "    Dense(20, activation='softmax')  # Set output dimensionality to 20\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 2s 9ms/step - loss: 2.9866 - accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.9389 - accuracy: 0.2000\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.8617 - accuracy: 0.2000\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.7454 - accuracy: 0.2000\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.6562 - accuracy: 0.2000\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.6243 - accuracy: 0.2000\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.5810 - accuracy: 0.2000\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.5496 - accuracy: 0.2000\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.5088 - accuracy: 0.2000\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.4659 - accuracy: 0.2062\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.4190 - accuracy: 0.2250\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.3646 - accuracy: 0.2812\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.2966 - accuracy: 0.3313\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.2106 - accuracy: 0.3688\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.1219 - accuracy: 0.3812\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.0534 - accuracy: 0.4000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.9590 - accuracy: 0.4125\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.8663 - accuracy: 0.4563\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.7492 - accuracy: 0.4875\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.6919 - accuracy: 0.4938\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.6028 - accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.4970 - accuracy: 0.5562\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.4729 - accuracy: 0.5312\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4068 - accuracy: 0.5750\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3340 - accuracy: 0.5875\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.2417 - accuracy: 0.6313\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1507 - accuracy: 0.6562\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0848 - accuracy: 0.6750\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0180 - accuracy: 0.7125\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.9384 - accuracy: 0.7375\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8577 - accuracy: 0.7688\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8103 - accuracy: 0.7875\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7278 - accuracy: 0.8062\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6756 - accuracy: 0.8250\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6136 - accuracy: 0.8562\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5643 - accuracy: 0.8750\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5245 - accuracy: 0.8938\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4773 - accuracy: 0.9125\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4333 - accuracy: 0.9187\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4005 - accuracy: 0.9312\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3727 - accuracy: 0.9312\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3473 - accuracy: 0.9312\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3265 - accuracy: 0.9500\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2895 - accuracy: 0.9500\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2778 - accuracy: 0.9625\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2585 - accuracy: 0.9625\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2339 - accuracy: 0.9625\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2307 - accuracy: 0.9563\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2075 - accuracy: 0.9688\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1884 - accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e3d2b43460>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 592ms/step - loss: 0.1536 - accuracy: 0.9688\n",
      "Test Loss: 0.15355908870697021\n",
      "Test Accuracy: 0.96875\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 592ms/step\n",
      "Accuracy: 0.96875\n",
      "Predicted: ✌️ Actual: ✌️\n",
      "Predicted: 😕 Actual: 😕\n",
      "Predicted: 👍 Actual: 👍\n",
      "Predicted: 😩 Actual: 😩\n",
      "Predicted: 😂 Actual: 😂\n",
      "Predicted: 😬 Actual: 😬\n",
      "Predicted: 😴 Actual: 😴\n",
      "Predicted: 😠 Actual: 😕\n",
      "Predicted: 🤔 Actual: 🤔\n",
      "Predicted: 🙏 Actual: 🙏\n",
      "Predicted: 😲 Actual: 😲\n",
      "Predicted: ✌️ Actual: ✌️\n",
      "Predicted: 😩 Actual: 😩\n",
      "Predicted: 😠 Actual: 😠\n",
      "Predicted: 😭 Actual: 😭\n",
      "Predicted: 😬 Actual: 😬\n",
      "Predicted: 😕 Actual: 😕\n",
      "Predicted: 🍹 Actual: 🍹\n",
      "Predicted: 😠 Actual: 😠\n",
      "Predicted: 😬 Actual: 😬\n",
      "Predicted: 😠 Actual: 😠\n",
      "Predicted: 😩 Actual: 😩\n",
      "Predicted: ✨ Actual: ✨\n",
      "Predicted: 😬 Actual: 😬\n",
      "Predicted: 😩 Actual: 😩\n",
      "Predicted: ✌️ Actual: ✌️\n",
      "Predicted: 😩 Actual: 😩\n",
      "Predicted: 😬 Actual: 😬\n",
      "Predicted: 🤔 Actual: 🤔\n",
      "Predicted: 🤔 Actual: 🤔\n",
      "Predicted: 😇 Actual: 😇\n",
      "Predicted: 🙏 Actual: 🙏\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predicted and actual labels from one-hot encoded format to integer labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "Y_test_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compare predicted labels with actual labels\n",
    "correct_predictions = np.sum(y_pred_labels == Y_test_labels)\n",
    "total_predictions = len(Y_test_labels)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display predicted labels and actual labels\n",
    "for i in range(len(y_pred_labels)):\n",
    "    print(\"Predicted:\", label_to_emoji(y_pred_labels[i]), \"Actual:\", label_to_emoji(Y_test_labels[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 570ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "test = [\"I am trying\", \"I want to cry\", \"This is just sad\"]\n",
    "test_seq = tokenizer.texts_to_sequences(test)\n",
    "Xtest = pad_sequences(test_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "y_pred = model.predict(Xtest)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am trying 😭\n",
      "I want to cry 😭\n",
      "This is just sad 😩\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test)):\n",
    "    print(test[i], label_to_emoji(y_pred[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
