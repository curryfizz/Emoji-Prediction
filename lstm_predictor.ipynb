{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Embedding\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split  # Import for splitting the data\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When your alarm goes off for the fifth time</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>That moment when someone eats the last slice o...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you finally finish a project</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me trying to understand the group chat</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When the music is too loud at the party</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   1\n",
       "0       When your alarm goes off for the fifth time    9\n",
       "1  That moment when someone eats the last slice o...   7\n",
       "2                 When you finally finish a project    8\n",
       "3            Me trying to understand the group chat   11\n",
       "4           When the music is too loud at the party   18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/emoji_data/emoji_data.csv', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emoji Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\n",
    "    0: \":red_heart:\",  # Love\n",
    "    1: \":face_with_tears_of_joy:\",  # Laughter\n",
    "    2: \":smiling_face_with_heart-eyes:\",  # Adoration\n",
    "    3: \":loudly_crying_face:\",  # Sadness\n",
    "    4: \":fire:\",  # Excitement\n",
    "    5: \":thumbs_up:\",  # Approval\n",
    "    6: \":folded_hands:\",  # Gratitude\n",
    "    7: \":angry_face:\",  # Anger\n",
    "    8: \":sparkles:\",  # Happiness\n",
    "    9: \":weary_face:\",  # Exhaustion\n",
    "    10: \":astonished_face:\",  # Surprise\n",
    "    11: \":confused_face:\",  # Confusion\n",
    "    12: \":tropical_drink:\",  # Celebration\n",
    "    13: \":broken_heart:\",  # Heartbreak\n",
    "    14: \":thinking_face:\",  # Contemplation\n",
    "    15: \":sleeping_face:\",  # Sleepiness\n",
    "    16: \":victory_hand:\",  # Success\n",
    "    17: \":thumbs_down:\",  # Disapproval\n",
    "    18: \":grimacing_face:\",  # Discomfort\n",
    "    19: \":smiling_face_with_halo:\",  # Innocence\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['When your alarm goes off for the fifth time ',\n",
       "       'That moment when someone eats the last slice of pizza ',\n",
       "       'When you finally finish a project ',\n",
       "       'Me trying to understand the group chat ',\n",
       "       'When the music is too loud at the party ',\n",
       "       'That feeling when you see your crush ',\n",
       "       'When dessert ruins your healthy eating plans ',\n",
       "       'Waking up on Monday morning ',\n",
       "       \"Pretending to laugh at your boss's joke \",\n",
       "       'Pretending to listen to a long story ',\n",
       "       \"Finding out there's free food at work \", 'Stuck in traffic ',\n",
       "       'Treating yourself even when broke ',\n",
       "       'Trying to adult but failing ',\n",
       "       'Trying to stay awake during a boring lecture ',\n",
       "       \"Realizing it's only Wednesday \",\n",
       "       'Trying to be positive but life keeps testing you ',\n",
       "       'Making plans for the weekend but ending up in bed ',\n",
       "       'Being the only one not having fun at a party ',\n",
       "       \"Accidentally liking someone's old post \",\n",
       "       'Figuring out what to wear in the morning ',\n",
       "       'Pretending to be interested in a boring story ',\n",
       "       'Forgetting to do something important ',\n",
       "       'Trying to get through the week ',\n",
       "       'Holding back tears but failing ',\n",
       "       'Finally relaxing after finishing chores ',\n",
       "       'Staying calm in a stressful situation ',\n",
       "       'Being productive but procrastinating instead ',\n",
       "       'Realizing you sent a text to the wrong person ',\n",
       "       'Resisting the urge to buy something unnecessary ',\n",
       "       'Trying to be positive but everything goes wrong ',\n",
       "       'Stuck in a never-ending meeting ',\n",
       "       'Being patient when someone is testing your limits ',\n",
       "       'Hiding disappointment behind a smile ',\n",
       "       'Realizing you have to wake up early on a weekend ',\n",
       "       \"Pretending to be excited about something you don't care about \",\n",
       "       'Trying to act cool but secretly freaking out inside ',\n",
       "       'Monday morning struggles ',\n",
       "       'When your phone dies mid-conversation ',\n",
       "       'Pretending to laugh at a bad joke ',\n",
       "       'Trying to act cool in front of your crush ',\n",
       "       \"When you accidentally like your ex's post \",\n",
       "       'Getting stuck in an awkward silence ',\n",
       "       'When you try to parallel park and fail ',\n",
       "       'Trying to look busy at work ',\n",
       "       'Realizing you forgot your lunch at home ',\n",
       "       'Sneezing in public and feeling judged ',\n",
       "       \"When you think it's Friday but it's only Thursday \",\n",
       "       'Waking up with bedhead and no time to fix it ',\n",
       "       \"When you forget someone's name right after meeting them \",\n",
       "       'Trying to resist hitting the snooze button ',\n",
       "       \"When you're hungry but can't decide what to eat \",\n",
       "       'When your WiFi suddenly stops working ',\n",
       "       'Accidentally sending a text to your boss instead of your friend ',\n",
       "       'Realizing you missed your favorite TV show ',\n",
       "       \"When you think it's Friday but it's only Monday \",\n",
       "       'Trying to hold back a yawn in a meeting ',\n",
       "       'Forgetting your umbrella on a rainy day ',\n",
       "       'When autocorrect changes your text to something embarrassing ',\n",
       "       'Sneezing multiple times in a row ',\n",
       "       \"When you're late for work and can't find your keys \",\n",
       "       'Accidentally dropping your phone face down ',\n",
       "       'When you see a spider in your room ',\n",
       "       'Forgetting your lunch and having to buy expensive cafeteria food ',\n",
       "       'When your friend cancels plans at the last minute ',\n",
       "       'Trying to make a decision but ending up more confused ',\n",
       "       'Realizing you left your wallet at home while grocery shopping ',\n",
       "       'When you accidentally hit \"reply all\" in an email ',\n",
       "       \"Pretending to understand a joke you didn't hear \",\n",
       "       'Trying to take a cute selfie and failing ',\n",
       "       'When your favorite song comes on the radio ',\n",
       "       'Getting a cramp while working out ',\n",
       "       \"When you're craving a snack but have nothing in the house \",\n",
       "       'Waking up to the sound of construction outside ',\n",
       "       'When your computer freezes in the middle of an important task ',\n",
       "       'Realizing you have no clean socks left ',\n",
       "       'Another Monday another thrilling start to the week',\n",
       "       'Who needs an umbrella when you have the power of optimism?',\n",
       "       'Plans canceled? Oh what a surprise!',\n",
       "       'Decoding the mysteries of pet behavior',\n",
       "       'Operating at peak brain capacity',\n",
       "       'Accidentally using the perfect emoji once again',\n",
       "       'Boss text mishap strikes again',\n",
       "       \"Logical reasoning? Nah I'll pass.\",\n",
       "       'Achieving superhero status one mundane task at a time',\n",
       "       'Craving ice cream at midnight? Well obviously.',\n",
       "       'Just another flawless interaction with my crush',\n",
       "       'Making up lyrics because who needs accuracy?',\n",
       "       'Reliving embarrassing moments in exquisite detail',\n",
       "       'Cereal: the dinner of champions.',\n",
       "       'Left on read again? Shocking I know.',\n",
       "       'Just another day in the endless abyss of the week',\n",
       "       \"Cracking the code of my friend's unique humor\",\n",
       "       \"Accidentally liking someone's post from 2015? Classic.\",\n",
       "       'Suppressing laughter: an art form mastered.',\n",
       "       'Naps: because who needs energy?',\n",
       "       'No such thing as too much regret over pizza choices.',\n",
       "       'Another exam another energy drink.',\n",
       "       'Who needs sleep? Energy drinks got me covered.',\n",
       "       'Exams canceled? More time for caffeine.',\n",
       "       'Exam questions: decipher or die.', 'Caffeine-fueled desperation.',\n",
       "       'Accidentally sent my energy drink order to the professor. Oops.',\n",
       "       'Boss text mishap. Just add it to the chaos.',\n",
       "       'Logical reasoning? More like panic mode.',\n",
       "       'Surviving on caffeine and hope.',\n",
       "       'Midnight craving: energy drinks. Sleep can wait.',\n",
       "       'Library vibes: energy drinks and anxiety.',\n",
       "       'Excuse for missing class: energy drink overdose.',\n",
       "       'Spilled energy drink on my exam. Great start.',\n",
       "       'Energy drinks: student fuel.',\n",
       "       'Left on read again? Story of my life.',\n",
       "       'Balancing exams and caffeine crashes.',\n",
       "       'Cracking open another can of energy drink.',\n",
       "       'Regretting energy drinks before bed.',\n",
       "       'Suppressing a caffeine-induced panic attack.',\n",
       "       'Naps: escape from the caffeine jitters.',\n",
       "       'No such thing as too much caffeine regret.',\n",
       "       \"Bae's text got me feeling all\",\n",
       "       'Laughing so hard tears streaming down my face',\n",
       "       'That sunset view got me like',\n",
       "       'Feeling broken inside but pretending to be okay',\n",
       "       'This party is lit',\n",
       "       'Giving your friend a thumbs up for that sick dance move',\n",
       "       'Sending prayers and good vibes',\n",
       "       \"When you're angry but trying to keep calm\",\n",
       "       'Just found out I aced the test!',\n",
       "       'Feeling astonished by the surprise birthday party',\n",
       "       \"Wait what? I'm so confused right now\",\n",
       "       'Time to celebrate with a tropical drink!',\n",
       "       'Heartbroken over the breakup',\n",
       "       \"Contemplating life's mysteries...\",\n",
       "       'Ready to crash after a long day',\n",
       "       'Victory dance because I nailed the presentation!',\n",
       "       'Disapproving of that awful movie',\n",
       "       'This discomfort is killing me',\n",
       "       'Feeling innocent in this chaotic world', 'OMG so cringe rn',\n",
       "       'That view tho', 'Feeling low-key shook', \"This party's lit af\",\n",
       "       'Giving props for that sick move', 'Sending good vibes your way',\n",
       "       \"When you're low-key angry but tryna chill\",\n",
       "       \"Just aced that test let's flex\", 'Shook by the surprise party',\n",
       "       'Wait what? So confused rn',\n",
       "       'Time to turn up with a tropical drink',\n",
       "       'Heartbroken over the breakup', \"Contemplating life's mysteries\",\n",
       "       'Ready to crash after a long day',\n",
       "       'Victory dance for nailing the presentation',\n",
       "       'Disapproving of that movie like', 'This discomfort is too much',\n",
       "       'Feeling innocent in this mess', 'I cry', 'Pajn', 'Mondays = pajn',\n",
       "       'i smol', 'ion understand anything'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[0].values\n",
    "Y = data[1].values\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/glove_dataset/glove.6B.100d.txt','r', encoding='utf8') as file:\n",
    "#     content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model_path = \"data/fast_text/crawl_dataset/crawl-300d-2M-subword.vec\"\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert FastText embeddings to a dictionary\n",
    "embeddings = {}\n",
    "for word in fasttext_model.index_to_key:\n",
    "    embeddings[word] = fasttext_model.get_vector(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = {}\n",
    "\n",
    "# for line in content:\n",
    "#     line = line.split()\n",
    "#     embeddings[line[0]] = np.array(line[1:], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert input text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_to_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtokens = tokenizer.texts_to_sequences(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_maxlen(data):\n",
    "    maxlen = 0\n",
    "    for sent in data:\n",
    "        maxlen = max(maxlen, len(sent))\n",
    "    \n",
    "    return maxlen\n",
    "maxlen = get_maxlen(Xtokens)\n",
    "\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = pad_sequences(Xtokens, maxlen=maxlen, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = to_categorical(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "128\n",
      "32\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(X_test))\n",
    "print(len(X_train))\n",
    "print(len(Y_test))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_to_index) + 1, embed_size))\n",
    "\n",
    "for word, i in word_to_index.items():\n",
    "    if word in embeddings:\n",
    "        embed_vector = embeddings[word]\n",
    "        embedding_matrix[i] = embed_vector\n",
    "    else:\n",
    "        # Handle out-of-vocabulary words or phrases by aggregating subword embeddings\n",
    "        phrase_embed_sum = None\n",
    "        for subword in word.split():\n",
    "            if subword in embeddings:\n",
    "                if phrase_embed_sum is None:\n",
    "                    phrase_embed_sum = embeddings[subword]\n",
    "                else:\n",
    "                    phrase_embed_sum += embeddings[subword]\n",
    "        if phrase_embed_sum is not None:\n",
    "            # Take the average of subword embeddings\n",
    "            embedding_matrix[i] = phrase_embed_sum / len(word.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 19:50:33.949772: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 19:50:34.033922: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 19:50:34.034374: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 19:50:34.038183: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 19:50:34.038753: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 19:50:34.039023: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 19:50:34.155868: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 19:50:34.156233: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 19:50:34.156496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-24 19:50:34.157205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2285 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-02-24 19:50:34.946607: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_to_index) + 1,\n",
    "              output_dim=embed_size,\n",
    "              input_length=maxlen,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False),\n",
    "    LSTM(units=50, return_sequences=True),\n",
    "    LSTM(units=50),\n",
    "    Dense(20, activation='softmax')  # Set output dimensionality to 20\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 19:50:43.264842: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-02-24 19:50:43.698097: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fbcf2a65720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-24 19:50:43.698147: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-02-24 19:50:43.719212: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708800643.973583   51566 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 5s 12ms/step - loss: 2.9788 - accuracy: 0.0625\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9157 - accuracy: 0.2000\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.7968 - accuracy: 0.2000\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6448 - accuracy: 0.2000\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6467 - accuracy: 0.2000\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.5927 - accuracy: 0.2000\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5791 - accuracy: 0.2000\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5488 - accuracy: 0.2000\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5114 - accuracy: 0.2000\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.4881 - accuracy: 0.2000\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.4394 - accuracy: 0.2125\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3939 - accuracy: 0.2438\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3392 - accuracy: 0.2750\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3011 - accuracy: 0.2750\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2383 - accuracy: 0.3000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.1362 - accuracy: 0.3375\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.0630 - accuracy: 0.3438\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.9830 - accuracy: 0.4062\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.9074 - accuracy: 0.4187\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.8007 - accuracy: 0.4500\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7144 - accuracy: 0.4875\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.6352 - accuracy: 0.5125\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6124 - accuracy: 0.4938\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.5217 - accuracy: 0.5562\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4142 - accuracy: 0.6187\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3418 - accuracy: 0.5938\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2658 - accuracy: 0.6250\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2157 - accuracy: 0.6562\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.1616 - accuracy: 0.6562\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0454 - accuracy: 0.6938\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0063 - accuracy: 0.7063\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.9445 - accuracy: 0.7188\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.8827 - accuracy: 0.7312\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.8207 - accuracy: 0.7688\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7718 - accuracy: 0.7688\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7160 - accuracy: 0.8062\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6591 - accuracy: 0.8562\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6136 - accuracy: 0.8562\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5593 - accuracy: 0.8875\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5281 - accuracy: 0.9062\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4981 - accuracy: 0.9250\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4491 - accuracy: 0.9250\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4116 - accuracy: 0.9375\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3818 - accuracy: 0.9563\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3530 - accuracy: 0.9563\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3218 - accuracy: 0.9625\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2917 - accuracy: 0.9688\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.2657 - accuracy: 0.9750\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2434 - accuracy: 0.9750\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2410 - accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fbdf8162da0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 914ms/step - loss: 0.2543 - accuracy: 0.9688\n",
      "Test Loss: 0.25428861379623413\n",
      "Test Accuracy: 0.96875\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Accuracy: 0.96875\n",
      "Predicted: üôè Actual: üëç\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predicted and actual labels from one-hot encoded format to integer labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "Y_test_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compare predicted labels with actual labels\n",
    "correct_predictions = np.sum(y_pred_labels == Y_test_labels)\n",
    "total_predictions = len(Y_test_labels)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display predicted labels and actual labels\n",
    "for i in range(len(y_pred_labels)):\n",
    "    if y_pred_labels[i]!=Y_test_labels[i]:\n",
    "        print(\"Predicted:\", label_to_emoji(y_pred_labels[i]), \"Actual:\", label_to_emoji(Y_test_labels[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 992ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "test = [\"I am trying\", \"I want to cry\", \"This is just sad\"]\n",
    "test_seq = tokenizer.texts_to_sequences(test)\n",
    "Xtest = pad_sequences(test_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "y_pred = model.predict(Xtest)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am trying üò≠\n",
      "I want to cry üò≠\n",
      "This is just sad üò¨\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test)):\n",
    "    print(test[i], label_to_emoji(y_pred[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
